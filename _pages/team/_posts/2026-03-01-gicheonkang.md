---
layout: member
category: faculty
title: 'Gi-Cheon Kang'
image: 'gckang.jpg'
role: Assistant Professor
affiliation: Dept. of Software and Computer Engineering
email: gchnkang@gmail.com
permalink: 'team/gckang'
social:
    cv: /assets/cv/CV_GCKANG.pdf
    github: https://github.com/gicheonkang
    website: https://gicheonkang.com
    google-scholar: https://scholar.google.com/citations?user=C6J7zI0AAAAJ&hl=en
---

### Bio

I am an Assistant Professor in the Department of [Software and Computer Engineering](http://software.ajou.ac.kr/e_main.php) at [Ajou University](https://ajou.ac.kr/en/index.do). Prior to joining Ajou University, I was a Postdoctoral Fellow at the AI Institute for Seoul National University ([AIIS](https://aiis.snu.ac.kr/eng/)). In 2025, I earned my Ph.D. in AI from [Seoul National University](https://en.snu.ac.kr/index.html) under the supervision of [Prof. Byoung-Tak Zhang](https://bi.snu.ac.kr/members/byoung-tak-zhang.html). I was selected as [RSS Pioneers](https://sites.google.com/view/rsspioneers2025/participants?authuser=0) in 2025 and as a [Youlchon AI Star Fellow](https://aiis.snu.ac.kr/bbs/board.php?bo_table=sub4_3&sca=2024) in 2024. 

My research lies at the intersection of multimodal AI and robotics. Specifically, I aim to advance robot learning and human-robot interaction by leveraging semantic, generalizable priors from multimodal AI models.

---

### News

<ul id="news-list">
  <li>[Nov 2025] I gave a talk at Dept. of Immersive Media Engineering at Sungkyunkwan University (SKKU).</li>
  <li>[Oct 2025] I gave a talk at UNIST graduate school of AI.</li>
  <li>[July 2025] Our vision-language navigation work is accepted to BMVC 2025.</li>
  <li>[April 2025] Happy to announce that our work (<a href="https://clip-rt.github.io">CLIP-RT</a>) is accepted to <a href="https://roboticsconference.org">RSS 2025</a>!</li>
  <li>[April 2025] I'm selected as a member of <a href="https://sites.google.com/view/rsspioneers2025/">RSS Pioneers</a>!</li>
  <li>[Nov 2024] Happy to release a new robotics foundation model, <a href="https://clip-rt.github.io">CLIP-RT</a>!</li>
  <li>[Aug 2024] I'm selected as a recipient of the Youlchon AI Star Fellowship.</li>
  <li>[Jun 2024] <a href="https://arxiv.org/abs/2310.12547">PGA</a> is accepted at <a href="https://iros2024-abudhabi.org">IROS 2024</a>.</li>
  <li>[Apr 2024] A preprint for embodied instruction following (<a href="https://arxiv.org/abs/2404.15190">Socratic Planner</a>) is released.</li>
  <li>[Mar 2024] I wrote my <a target="_blank" href="docs/Research_Statement_GCKANG.pdf">research statement</a> about what I've been studying.</li>
  <li>[Mar 2024] A new preprint (<a href="https://arxiv.org/abs/2403.15049">Continual Vision-and-Language Navigation</a>) is released.</li>
  <li>[Jan 2024] <a href="https://arxiv.org/abs/2309.07759">PROGrasp</a> is accepted to <a href="https://2024.ieee-icra.org">ICRA 2024</a>!</li>
  <li>[Dec 2023] I attend <a href="https://sites.google.com/g.skku.edu/brainlink2023/">Brainlink 2023</a>.</li>
  <li>[Nov 2023] I'll give a talk at Dept. of Energy Resources Engineering at Seoul National University (Title: "The Evolution of Language Models: From Basic NLP to ChatGPT and Beyond").</li>
  <li>[Oct 2023] Two preprints (<a href="https://arxiv.org/abs/2309.07759">PROGrasp</a> and <a href="https://arxiv.org/abs/2310.12547">PGA</a>) are released!</li>
  <li>[Jun 2023] One paper is accepted to <a href="https://ieee-iros.org">IROS 2023</a>!</li>
  <li>[Mar 2023] Happy to announce that <a href="https://arxiv.org/abs/2205.12502">our paper</a> is accepted to <a href="https://cvpr2023.thecvf.com">CVPR 2023</a>!</li>
  <li>[Jun 2022] One paper is accepted to ICML 2022 <a href="https://pretraining.github.io">Pre-training Workshop</a>.</li>
  <li>[May 2022] Thrilled to announce that our <a href="https://arxiv.org/abs/2205.12502">new preprint</a> is released!</li>
  <li>[Apr 2022] One paper is accepted to CVPR 2022 <a href="https://sites.google.com/nycu.edu.tw/hcis/home">HCIS Workshop</a>.</li>
  <li>[Dec 2021] I gave an invited talk at Korea Software Congress.</li>
  <li>[Oct 2021] One paper is accepted to NeurIPS 2021 CtrlGen Workshop.</li>
  <li>[Aug 2021] One paper is accepted to Findings of EMNLP 2021.</li>
  <li>[May 2021] One paper is accepted to ACL 2021.</li>
  <li>[Sep 2020] I'm starting my Ph.D. in this fall.</li>
  <li>[Jun 2020] From July, I'll join <a href="https://aiis.snu.ac.kr">SNU AI Institute</a> (AIIS) as a researcher.</li>
  <li>[Jan 2020] Our paper has been accepted to ICASSP 2020!</li>
  <li>[Dec 2019] From January, I'll be a research intern at <a href="https://www.skt.ai">SK T-Brain</a>!</li>
  <li>[Nov 2019] I gave a spotlight talk at <a href="https://videoturingtest.github.io">Video Turing Test workshop</a>, ICCV 2019.</li>
  <li>[Oct 2019] I gave an invited talk at <a href="https://www.skt.ai">SK Telecom AI Center</a>.</li>
  <li>[Aug 2019] Excited to announce that <a href="https://arxiv.org/abs/1902.09368">our paper</a> has been accepted to <a href="https://www.emnlp-ijcnlp2019.org/">EMNLP 2019</a>.</li>
  <li>[Jun 2019] Our proposed method ranks <b>3rd place</b> in <a href="https://visualdialog.org/challenge/2019">Visual Dialog Challenge 2019</a>!!</li>
  <li>[Aug 2018] We have a paper accepted to ECCV 2018 Workshop on <a href="http://vizwiz.org/workshop/">VizWiz Grand Challenge</a>.</li>
</ul>
{% include news_readmore.html id="news" threshold="5" %}
---

### Recent Publications <span style="font-size: 16px; display: inline;">(<a href="/publications">Full List</a>)</span>

<div class="section-header">
  <h3>Recent Publications</h3>
  <span style="font-size: 16px; display: inline;">(<a href="/publications">Full List</a>)</span>
</div>

<ul id="pub">
  <li>Seongjun Jeong, Gi-Cheon Kang, Seongho Choi, Joochan Kim, Byoung-Tak Zhang,
    <a href="https://arxiv.org/abs/2403.15049">Continual Vision-and-Language Navigation</a>,
    in BMVC 2025
  </li>

  <li>Gi-Cheon Kang<sup>*</sup>, Junghyun Kim<sup>*</sup>, Kyuhwan Shim, Jun Ki Lee, Byoung-Tak Zhang,
    <a href="https://www.roboticsproceedings.org/rss21/p016.html">CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision</a>,
    in RSS 2025 (Oral)
  </li>

  <li>Suyeon Shin, Sujin Jeon<sup>*</sup>, Junghyun Kim<sup>*</sup>, Gi-Cheon Kang<sup>*</sup>, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/abstract/document/11128677">Socratic Planner: Self-QA-Based Zero-Shot Planning for Embodied Instruction Following</a>,
    in ICRA 2025
  </li>

  <li>Junghyun Kim, Gi-Cheon Kang<sup>*</sup>, Jaein Kim<sup>*</sup>, Seoyoon Yang, Minjoon Jung, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/document/10801347">PGA: Personalizing Grasping Agents with Single Human-Robot Interaction</a>,
    in IROS 2024
  </li>

  <li>Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/document/10610543">PROGrasp: Pragmatic Human-Robot Communication for Object Grasping</a>,
    in ICRA 2024 (Oral)
  </li>

  <li>Gi-Cheon Kang, Sungdong Kim<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>, Donghyun Kwak<sup>*</sup>, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/document/10203535">The Dialog Must Go On: Improving Visual Dialog via Generative Self-Training</a>,
    in CVPR 2023
  </li>

  <li>Junghyun Kim, Gi-Cheon Kang<sup>*</sup>, Jaein Kim<sup>*</sup>, Suyeon Shin, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/document/10342021">GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation</a>,
    in IROS 2023 (Oral)
  </li>

  <li>Gi-Cheon Kang, Junseok Park, Hwaran Lee, Byoung-Tak Zhang<sup>*</sup>, Jin-Hwa Kim<sup>*</sup>,
    <a href="https://aclanthology.org/2021.findings-emnlp.31/">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer</a>,
    in EMNLP Findings 2021
  </li>

  <li>Ahjeong Seo, Gi-Cheon Kang, Joonhan Park, Byoung-Tak Zhang,
    <a href="https://aclanthology.org/2021.acl-long.481/">Attend What You Need: Motion-Appearance Synergistic Networks for Video Question Answering</a>,
    in ACL 2021
  </li>

  <li>Taehyeong Kim, Injune Hwang, Gi-Cheon Kang, Won-Seok Choi, Hyunseo Kim, Byoung-Tak Zhang,
    <a href="https://ieeexplore.ieee.org/document/9054655">Label Propagation Adaptive Resonance Theory for Semi-Supervised Continuous Learning</a>,
    in ICASSP 2020
  </li>

  <li>Gi-Cheon Kang, Jaeseo Lim, Byoung-Tak Zhang,
    <a href="https://aclanthology.org/D19-1209/">Dual Attention Networks for Visual Reference Resolution in Visual Dialog</a>,
    in EMNLP 2019
  </li>
</ul>
{% include pub_btn.html id="pub" threshold="5" %}
